{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "countries = pd.read_csv(\"Countries1.csv\")\n",
    "country_codes = countries[\"Alpha2\"].tolist() # Get country code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Failed codes: ['JG', nan]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "base_url = \"https://data.countrydata.iatistandard.org/output/web/xlsx/en\"\n",
    "out_dir = Path(\"data/country_xlsx\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "failed = []\n",
    "\n",
    "for code in country_codes:\n",
    "    url = f\"{base_url}/{code}.xlsx\"\n",
    "    out_path = out_dir / f\"{code}.xlsx\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    except requests.exceptions.HTTPError:\n",
    "        failed.append(code)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Failed codes:\", failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IATI DATA CLEANING AND TRANSFORMATION\n",
      "============================================================\n",
      "Input directory: data/country_xlsx\n",
      "Output directory: data/cleaned_country_xlsx\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = Path(\"data/country_xlsx\")\n",
    "OUTPUT_DIR = Path(\"data/cleaned_country_xlsx\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns to remove\n",
    "COLUMNS_TO_REMOVE = ['Humanitarian', 'Multi Country', 'URL', 'Value (EUR)', 'Value (Local currrency)']\n",
    "\n",
    "# Expected date format for database\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "\n",
    "def clean_text_field(text): # Standardize text fields: trim whitespace, normalize encoding\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    text = str(text).strip()\n",
    "\n",
    "    # Handle 'No data' or similar placeholders\n",
    "    if text.lower() in ['no data', 'n/a', 'na', 'null', '']:\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_country_name(country):\n",
    "    # Normalize country spellings and names\n",
    "    # Common corrections for misspellings\n",
    "\n",
    "    if pd.isna(country):\n",
    "        return country\n",
    "\n",
    "    country = str(country).strip()\n",
    "\n",
    "    # Dictionary of common misspellings/variations\n",
    "    corrections = {\n",
    "        'Turkiye': 'Turkey',\n",
    "        'Türkiye': 'Turkey',\n",
    "        'Ivory Coast': \"Côte d'Ivoire\",\n",
    "        'Congo DRC': 'Democratic Republic of the Congo',\n",
    "        'Congo Republic': 'Republic of the Congo',\n",
    "    }\n",
    "\n",
    "    for wrong, correct in corrections.items():\n",
    "        if wrong.lower() in country.lower():\n",
    "            return correct\n",
    "\n",
    "    return country\n",
    "\n",
    "\n",
    "def validate_numeric_values(df):\n",
    "    # Validate that numeric fields have acceptable values\n",
    "    # Remove/correct impossible values (e.g., negative values where positive expected)\n",
    "\n",
    "    # Value columns should not be negative (assuming all transactions are positive)\n",
    "    value_columns = ['Value (USD)']\n",
    "\n",
    "    for col in value_columns:\n",
    "        if col in df.columns:\n",
    "            # Flag negative values\n",
    "            negative_mask = df[col] < 0\n",
    "            if negative_mask.any():\n",
    "                print(f\"  WARNING: Found {negative_mask.sum()} negative values in {col}\")\n",
    "                # Option 1: Set to absolute value\n",
    "                # df.loc[negative_mask, col] = df.loc[negative_mask, col].abs()\n",
    "                # Option 2: Set to NaN\n",
    "                df.loc[negative_mask, col] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df): #Remove duplicate rows based on key columns\n",
    "\n",
    "    # Define key columns that should be unique\n",
    "    # Rows that are duplicated in all these columns will be considered duplicates\n",
    "    key_columns = [\n",
    "        'IATI Identifier',\n",
    "        'Transaction Type',\n",
    "        'Calendar Year',\n",
    "        'Calendar Quarter',\n",
    "        'Value (USD)'\n",
    "    ]\n",
    "\n",
    "    # Check columns exist\n",
    "    existing_keys = [col for col in key_columns if col in df.columns]\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=existing_keys, keep='first')\n",
    "    removed = initial_count - len(df)\n",
    "\n",
    "    if removed > 0:\n",
    "        print(f\"  Removed {removed} duplicate rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_transform_data(file_path):\n",
    "    # Main cleaning and transformation function for a single country file\n",
    "\n",
    "    country_code = file_path.stem  # e.g., 'AF' from 'AF.xlsx'\n",
    "    print(f\"\\nProcessing: {country_code}.xlsx\")\n",
    "\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"  Initial rows: {len(df)}\")\n",
    "\n",
    "        # Remove specified columns\n",
    "        columns_to_drop = [col for col in COLUMNS_TO_REMOVE if col in df.columns]\n",
    "        if columns_to_drop:\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "            print(f\"  Removed columns: {', '.join(columns_to_drop)}\")\n",
    "\n",
    "        # Clean text fields\n",
    "        text_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].apply(clean_text_field)\n",
    "\n",
    "        # Normalize country names\n",
    "        if 'Recipient Country or Region' in df.columns:\n",
    "            df['Recipient Country or Region'] = df['Recipient Country or Region'].apply(\n",
    "                normalize_country_name\n",
    "            )\n",
    "\n",
    "        # Validate numeric values\n",
    "        df = validate_numeric_values(df)\n",
    "\n",
    "        # Remove duplicates\n",
    "        df = remove_duplicates(df)\n",
    "\n",
    "        # Handle missing values in key fields\n",
    "        critical_fields = ['IATI Identifier', 'Transaction Type', 'Calendar Year']\n",
    "        before_drop = len(df)\n",
    "        df = df.dropna(subset=critical_fields)\n",
    "        dropped = before_drop - len(df)\n",
    "        if dropped > 0:\n",
    "            print(f\"Dropped {dropped} rows with missing critical fields\")\n",
    "\n",
    "        # Sort by date and identifier\n",
    "        sort_columns = ['Calendar Year and Quarter', 'IATI Identifier']\n",
    "        existing_sort = [col for col in sort_columns if col in df.columns]\n",
    "        if existing_sort:\n",
    "            df = df.sort_values(by=existing_sort)\n",
    "\n",
    "        # Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Save cleaned data\n",
    "        output_file = OUTPUT_DIR / f\"{country_code}_cleaned.csv\"\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"  Final rows: {len(df)}\")\n",
    "        print(f\"  Saved to: {output_file.name}\")\n",
    "\n",
    "        return {\n",
    "            'country': country_code,\n",
    "            'initial_rows': len(df),\n",
    "            'final_rows': len(df),\n",
    "            'success': True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")\n",
    "        return {\n",
    "            'country': country_code,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Main execution function\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"IATI DATA CLEANING AND TRANSFORMATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input directory: {INPUT_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Get all Excel files (country files)\n",
    "    excel_files = list(INPUT_DIR.glob(\"*.xlsx\"))\n",
    "\n",
    "    if not excel_files:\n",
    "        print(f\"\\nERROR: File not found: {excel_files}\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    for file_path in excel_files:\n",
    "        # Build expected cleaned filename\n",
    "        cleaned_name = file_path.stem + \"_cleaned.csv\"\n",
    "        cleaned_path = OUTPUT_DIR / cleaned_name\n",
    "\n",
    "        if cleaned_path.exists():\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {file_path.name}\")\n",
    "        clean_and_transform_data(file_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILDING DIMENSIONAL MODEL FROM CLEANED DATA\n",
      "======================================================================\n",
      "Input: data/cleaned_country_xlsx\n",
      "Dimensions output: data/cleaned_country_xlsx/dimensions\n",
      "Facts output: data/cleaned_country_xlsx/facts\n",
      "\n",
      "======================================================================\n",
      "LOADING CLEANED CSV FILES\n",
      "======================================================================\n",
      "\n",
      "Found 230 cleaned files\n",
      "  Loaded UY_cleaned.csv: 18,455 rows\n",
      "  Loaded CN_cleaned.csv: 144,561 rows\n",
      "  Loaded LC_cleaned.csv: 11,282 rows\n",
      "  Loaded JP_cleaned.csv: 2,803 rows\n",
      "  Loaded IM_cleaned.csv: 24 rows\n",
      "  Loaded TJ_cleaned.csv: 54,464 rows\n",
      "  Loaded MP_cleaned.csv: 329 rows\n",
      "  Loaded PW_cleaned.csv: 4,884 rows\n",
      "  Loaded GL_cleaned.csv: 219 rows\n",
      "  Loaded JO_cleaned.csv: 105,701 rows\n",
      "\n",
      " Loaded 10 files successfully\n",
      "\n",
      "======================================================================\n",
      "CREATING DIMENSION TABLES\n",
      "======================================================================\n",
      "\n",
      "Creating dimTime DataFrame...\n",
      "   Created DataFrame with 11,323 rows\n",
      "\n",
      "Creating dimAidType DataFrame...\n",
      "   Created DataFrame with 18 rows\n",
      "\n",
      "Creating dimFinanceType DataFrame...\n",
      "   Created DataFrame with 12 rows\n",
      "\n",
      "Creating dimFlowType DataFrame...\n",
      "   Created DataFrame with 7 rows\n",
      "\n",
      "Creating dimTask DataFrame...\n",
      "   Created DataFrame with 19,579 rows\n",
      "\n",
      "Creating dimProviderOrg DataFrame...\n",
      "   Created DataFrame with 1,148 rows\n",
      "\n",
      "Creating dimReportingOrg DataFrame...\n",
      "   Created DataFrame with 278 rows\n",
      "\n",
      "Creating dimRecipientCountry DataFrame...\n",
      "   Created DataFrame with 10 rows\n",
      "\n",
      "Creating dimRecipientOrg DataFrame...\n",
      "   Created DataFrame with 7,804 rows\n",
      "\n",
      "Creating dimSector DataFrame...\n",
      "   Created DataFrame with 25 rows\n",
      "\n",
      "Creating dimSubSector DataFrame...\n",
      "   Created DataFrame with 267 rows\n",
      "\n",
      " Created 11 dimension tables\n",
      "\n",
      "======================================================================\n",
      "CREATING FACT TABLE: factAidTransactions\n",
      "======================================================================\n",
      "\n",
      "Combined 342,722 transactions\n",
      "\n",
      "Mapping foreign keys...\n",
      "\n",
      " Created fact table DataFrame with 2,227,873 rows\n",
      "\n",
      "======================================================================\n",
      "COMPLETE!\n",
      "======================================================================\n",
      "\n",
      " All tables created in:\n",
      "  Dimensions: data/cleaned_country_xlsx/dimensions\n",
      "  Facts: data/cleaned_country_xlsx/facts\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = Path(\"data/cleaned_country_xlsx\")\n",
    "DIMENSION_DIR = INPUT_DIR / \"dimensions\"\n",
    "FACT_DIR = INPUT_DIR / \"facts\"\n",
    "\n",
    "# Create directories\n",
    "DIMENSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING DIMENSIONAL MODEL FROM CLEANED DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Input: {INPUT_DIR}\")\n",
    "print(f\"Dimensions output: {DIMENSION_DIR}\")\n",
    "print(f\"Facts output: {FACT_DIR}\\n\")\n",
    "\n",
    "# LOAD ALL CLEANED CSV FILES\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CLEANED CSV FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all cleaned country CSV files\n",
    "cleaned_files = list(INPUT_DIR.glob(\"*_cleaned.csv\"))\n",
    "\n",
    "if not cleaned_files:\n",
    "    print(\"\\nERROR: No cleaned CSV files found!\")\n",
    "    print(\"Please run the cleaning script first.\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nFound {len(cleaned_files)} cleaned files\")\n",
    "\n",
    "# Load all cleaned files into a list\n",
    "all_cleaned_data = []\n",
    "for file in cleaned_files[:10]:\n",
    "    df = pd.read_csv(file)\n",
    "    all_cleaned_data.append(df)\n",
    "    print(f\"  Loaded {file.name}: {len(df):,} rows\")\n",
    "\n",
    "print(f\"\\n Loaded {len(all_cleaned_data)} files successfully\")\n",
    "\n",
    "# Rest of the code is exactly the same as document 2...\n",
    "# (Copy all the dimension creation functions and main() from document 2)\n",
    "\n",
    "# DIMENSION TABLE CREATION FUNCTIONS\n",
    "\n",
    "def create_dim_time(all_data_list):\n",
    "    print(\"\\nCreating dimTime DataFrame...\")\n",
    "    dates = pd.date_range(start='2000-01-01', end='2030-12-31', freq='D')\n",
    "\n",
    "    dim_time = pd.DataFrame({\n",
    "        'Time_Key': dates.strftime('%Y%m%d').astype(int),\n",
    "        'Calendar_Year': dates.year,\n",
    "        'Calendar_Quarter': 'Q' + dates.quarter.astype(str),\n",
    "        'Year_Quarter_Label': dates.year.astype(str) + ' Q' + dates.quarter.astype(str)\n",
    "    })\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimTime.csv\"\n",
    "    dim_time.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_time):,} rows\")\n",
    "    return dim_time\n",
    "\n",
    "\n",
    "def create_dim_aid_type(all_data_list):\n",
    "    print(\"\\nCreating dimAidType DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_aid_type = combined[['Aid Type']].drop_duplicates()\n",
    "    dim_aid_type = dim_aid_type[dim_aid_type['Aid Type'].notna()]\n",
    "    dim_aid_type = dim_aid_type.rename(columns={'Aid Type': 'Aid_Type'})\n",
    "    dim_aid_type['Aid_Type_Key'] = range(1, len(dim_aid_type) + 1)\n",
    "    dim_aid_type = dim_aid_type[['Aid_Type_Key', 'Aid_Type']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimAidType.csv\"\n",
    "    dim_aid_type.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_aid_type):,} rows\")\n",
    "    return dim_aid_type\n",
    "\n",
    "\n",
    "def create_dim_finance_type(all_data_list):\n",
    "    print(\"\\nCreating dimFinanceType DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_finance_type = combined[['Finance Type']].drop_duplicates()\n",
    "    dim_finance_type = dim_finance_type[dim_finance_type['Finance Type'].notna()]\n",
    "    dim_finance_type = dim_finance_type.rename(columns={'Finance Type': 'Finance_Category'})\n",
    "    dim_finance_type['Finance_Type_Key'] = range(1, len(dim_finance_type) + 1)\n",
    "    dim_finance_type = dim_finance_type[['Finance_Type_Key', 'Finance_Category']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimFinanceType.csv\"\n",
    "    dim_finance_type.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_finance_type):,} rows\")\n",
    "    return dim_finance_type\n",
    "\n",
    "\n",
    "def create_dim_flow_type(all_data_list):\n",
    "    print(\"\\nCreating dimFlowType DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_flow_type = combined[['Flow Type']].drop_duplicates()\n",
    "    dim_flow_type = dim_flow_type[dim_flow_type['Flow Type'].notna()]\n",
    "    dim_flow_type = dim_flow_type.rename(columns={'Flow Type': 'Flow_Category'})\n",
    "    dim_flow_type['Flow_Type_Key'] = range(1, len(dim_flow_type) + 1)\n",
    "    dim_flow_type = dim_flow_type[['Flow_Type_Key', 'Flow_Category']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimFlowType.csv\"\n",
    "    dim_flow_type.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_flow_type):,} rows\")\n",
    "    return dim_flow_type\n",
    "\n",
    "\n",
    "def create_dim_task(all_data_list):\n",
    "    print(\"\\nCreating dimTask DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_task = combined[['IATI Identifier', 'Title']].drop_duplicates()\n",
    "    dim_task = dim_task[dim_task['IATI Identifier'].notna()]\n",
    "    dim_task = dim_task.rename(columns={\n",
    "        'IATI Identifier': 'IATI_Identifier',\n",
    "        'Title': 'Title'\n",
    "    })\n",
    "    dim_task['Task_Key'] = range(1, len(dim_task) + 1)\n",
    "    dim_task = dim_task[['Task_Key', 'Title', 'IATI_Identifier']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimTask.csv\"\n",
    "    dim_task.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_task):,} rows\")\n",
    "    return dim_task\n",
    "\n",
    "\n",
    "def create_dim_provider_org(all_data_list):\n",
    "    print(\"\\nCreating dimProviderOrg DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_provider_org = combined[['Provider Organisation', 'Provider Organisation Type']].drop_duplicates()\n",
    "    dim_provider_org = dim_provider_org[dim_provider_org['Provider Organisation'].notna()]\n",
    "    dim_provider_org = dim_provider_org.rename(columns={\n",
    "        'Provider Organisation': 'Provider_Org',\n",
    "        'Provider Organisation Type': 'Provider_Org_Type'\n",
    "    })\n",
    "    dim_provider_org['Provider_Org_Key'] = range(1, len(dim_provider_org) + 1)\n",
    "    dim_provider_org = dim_provider_org[['Provider_Org_Key', 'Provider_Org', 'Provider_Org_Type']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimProviderOrg.csv\"\n",
    "    dim_provider_org.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_provider_org):,} rows\")\n",
    "    return dim_provider_org\n",
    "\n",
    "\n",
    "def create_dim_reporting_org(all_data_list):\n",
    "    print(\"\\nCreating dimReportingOrg DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_reporting_org = combined[[\n",
    "        'Reporting Organisation',\n",
    "        'Reporting Organisation Type',\n",
    "        'Reporting Organisation Group'\n",
    "    ]].drop_duplicates()\n",
    "    dim_reporting_org = dim_reporting_org[dim_reporting_org['Reporting Organisation'].notna()]\n",
    "    dim_reporting_org = dim_reporting_org.rename(columns={\n",
    "        'Reporting Organisation': 'Reporting_Org',\n",
    "        'Reporting Organisation Type': 'Reporting_Org_Type',\n",
    "        'Reporting Organisation Group': 'Reporting_Org_Group'\n",
    "    })\n",
    "    dim_reporting_org['Reporting_Org_Key'] = range(1, len(dim_reporting_org) + 1)\n",
    "    dim_reporting_org = dim_reporting_org[[\n",
    "        'Reporting_Org_Key',\n",
    "        'Reporting_Org',\n",
    "        'Reporting_Org_Type',\n",
    "        'Reporting_Org_Group'\n",
    "    ]]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimReportingOrg.csv\"\n",
    "    dim_reporting_org.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_reporting_org):,} rows\")\n",
    "    return dim_reporting_org\n",
    "\n",
    "\n",
    "def create_dim_recipient_country(all_data_list):\n",
    "    print(\"\\nCreating dimRecipientCountry DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_country = (\n",
    "        combined[['Recipient Country or Region']]\n",
    "        .drop_duplicates()\n",
    "        .dropna()\n",
    "        .rename(columns={'Recipient Country or Region': 'Country_Name'})\n",
    "    )\n",
    "\n",
    "    dim_country['iso_alpha2'] = None  # placeholder if not available\n",
    "\n",
    "    dim_country['Population'] = None\n",
    "    dim_country['Life_Expectancy'] = None\n",
    "    dim_country['GDP_USD'] = None\n",
    "\n",
    "    dim_country['Recipient_Country_Key'] = range(1, len(dim_country) + 1)\n",
    "\n",
    "    dim_country = dim_country[[\n",
    "        'Recipient_Country_Key',\n",
    "        'Country_Name',\n",
    "        'iso_alpha2',\n",
    "        'Population',\n",
    "        'Life_Expectancy',\n",
    "        'GDP_USD'\n",
    "    ]]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimRecipientCountry.csv\"\n",
    "    dim_country.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_country):,} rows\")\n",
    "\n",
    "    return dim_country\n",
    "\n",
    "\n",
    "\n",
    "def create_dim_recipient_org(all_data_list, dim_country):\n",
    "    print(\"\\nCreating dimRecipientOrg DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_recipient_org = combined[[\n",
    "        'Receiver Organisation',\n",
    "        'Receiver Organisation Type',\n",
    "        'Recipient Country or Region'\n",
    "    ]].drop_duplicates()\n",
    "\n",
    "    dim_recipient_org = dim_recipient_org[dim_recipient_org['Receiver Organisation'].notna()]\n",
    "    dim_recipient_org = dim_recipient_org.rename(columns={\n",
    "        'Receiver Organisation': 'Recipient_Org',\n",
    "        'Receiver Organisation Type': 'Recipient_Org_Type'\n",
    "    })\n",
    "\n",
    "    dim_recipient_org = dim_recipient_org.merge(\n",
    "        dim_country[['Recipient_Country_Key', 'Country_Name']],\n",
    "        left_on='Recipient Country or Region',\n",
    "        right_on='Country_Name',\n",
    "        how='left'\n",
    "    ).drop(columns=['Recipient Country or Region', 'Country_Name'])\n",
    "\n",
    "    dim_recipient_org['Recipient_Org_Key'] = range(1, len(dim_recipient_org) + 1)\n",
    "    dim_recipient_org = dim_recipient_org[[\n",
    "        'Recipient_Org_Key',\n",
    "        'Recipient_Org',\n",
    "        'Recipient_Org_Type',\n",
    "        'Recipient_Country_Key'\n",
    "    ]]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimRecipientOrg.csv\"\n",
    "    dim_recipient_org.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_recipient_org):,} rows\")\n",
    "    return dim_recipient_org\n",
    "\n",
    "\n",
    "def create_dim_sector(all_data_list):\n",
    "    print(\"\\nCreating dimSector DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_sector = combined[['Sector Category']].drop_duplicates()\n",
    "    dim_sector = dim_sector[dim_sector['Sector Category'].notna()]\n",
    "    dim_sector = dim_sector.rename(columns={'Sector Category': 'Sector_Category'})\n",
    "    dim_sector['Sector_Name'] = dim_sector['Sector_Category'].str.extract(r'^(\\d+)')\n",
    "\n",
    "    dim_sector['Sector_Key'] = range(1, len(dim_sector) + 1)\n",
    "    dim_sector = dim_sector[['Sector_Key', 'Sector_Name', 'Sector_Category']]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimSector.csv\"\n",
    "    dim_sector.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_sector):,} rows\")\n",
    "    return dim_sector\n",
    "\n",
    "\n",
    "def create_dim_sub_sector(all_data_list, dim_sector):\n",
    "    print(\"\\nCreating dimSubSector DataFrame...\")\n",
    "    combined = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "    dim_sub_sector = combined[['Sector', 'Sector Category']].drop_duplicates()\n",
    "    dim_sub_sector = dim_sub_sector[dim_sub_sector['Sector'].notna()]\n",
    "    dim_sub_sector = dim_sub_sector.rename(columns={\n",
    "        'Sector': 'Sub_Sector_Name',\n",
    "        'Sector Category': 'Sub_Sector_Category'\n",
    "    })\n",
    "\n",
    "    dim_sub_sector = dim_sub_sector.merge(\n",
    "        dim_sector[['Sector_Key', 'Sector_Category']],\n",
    "        left_on='Sub_Sector_Category',\n",
    "        right_on='Sector_Category',\n",
    "        how='left'\n",
    "    ).drop(columns=['Sector_Category'])\n",
    "\n",
    "    dim_sub_sector['Sub_Sector_Key'] = range(1, len(dim_sub_sector) + 1)\n",
    "    dim_sub_sector = dim_sub_sector[[\n",
    "        'Sub_Sector_Key',\n",
    "        'Sub_Sector_Name',\n",
    "        'Sub_Sector_Category',\n",
    "        'Sector_Key'\n",
    "    ]]\n",
    "\n",
    "    output_file = DIMENSION_DIR / \"dimSubSector.csv\"\n",
    "    dim_sub_sector.to_csv(output_file, index=False)\n",
    "    print(f\"   Created DataFrame with {len(dim_sub_sector):,} rows\")\n",
    "    return dim_sub_sector\n",
    "\n",
    "\n",
    "def create_fact_aid_transactions(all_data_list, dimensions):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING FACT TABLE: factAidTransactions\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    fact_table = pd.concat(all_data_list, ignore_index=True)\n",
    "    print(f\"\\nCombined {len(fact_table):,} transactions\")\n",
    "\n",
    "    # Split year and quarter\n",
    "    year_q = fact_table['Calendar Year and Quarter'].str.split(\" \", expand=True)\n",
    "\n",
    "    fact_table['Time_Key'] = (\n",
    "        year_q[0] + year_q[1].str.replace(\"Q\", \"\")\n",
    "    ).astype(int)\n",
    "\n",
    "    print(\"\\nMapping foreign keys...\")\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['aid_type'][['Aid_Type_Key', 'Aid_Type']],\n",
    "        left_on='Aid Type', right_on='Aid_Type', how='left'\n",
    "    ).drop(columns=['Aid_Type'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['finance_type'][['Finance_Type_Key', 'Finance_Category']],\n",
    "        left_on='Finance Type', right_on='Finance_Category', how='left'\n",
    "    ).drop(columns=['Finance_Category'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['flow_type'][['Flow_Type_Key', 'Flow_Category']],\n",
    "        left_on='Flow Type', right_on='Flow_Category', how='left'\n",
    "    ).drop(columns=['Flow_Category'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['task'][['Task_Key', 'IATI_Identifier']],\n",
    "        left_on='IATI Identifier', right_on='IATI_Identifier', how='left'\n",
    "    ).drop(columns=['IATI_Identifier'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['provider_org'][['Provider_Org_Key', 'Provider_Org']],\n",
    "        left_on='Provider Organisation', right_on='Provider_Org', how='left'\n",
    "    ).drop(columns=['Provider_Org'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['recipient_org'][['Recipient_Org_Key', 'Recipient_Org']],\n",
    "        left_on='Receiver Organisation', right_on='Recipient_Org', how='left'\n",
    "    ).drop(columns=['Recipient_Org'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['reporting_org'][['Reporting_Org_Key', 'Reporting_Org']],\n",
    "        left_on='Reporting Organisation', right_on='Reporting_Org', how='left'\n",
    "    ).drop(columns=['Reporting_Org'])\n",
    "\n",
    "    fact_table = fact_table.merge(\n",
    "        dimensions['sub_sector'][['Sub_Sector_Key', 'Sub_Sector_Name']],\n",
    "        left_on='Sector', right_on='Sub_Sector_Name', how='left'\n",
    "    ).drop(columns=['Sub_Sector_Name'])\n",
    "\n",
    "    fact_table['Aid_Fact_Key'] = range(1, len(fact_table) + 1)\n",
    "\n",
    "    fact_table = fact_table[[\n",
    "        'Aid_Fact_Key',\n",
    "        'Sub_Sector_Key',\n",
    "        'Task_Key',\n",
    "        'Reporting_Org_Key',\n",
    "        'Provider_Org_Key',\n",
    "        'Recipient_Org_Key',\n",
    "        'Aid_Type_Key',\n",
    "        'Finance_Type_Key',\n",
    "        'Flow_Type_Key',\n",
    "        'Time_Key',\n",
    "        'Value (USD)'\n",
    "    ]]\n",
    "\n",
    "    fact_table = fact_table.rename(columns={'Value (USD)': 'Value_USD'})\n",
    "\n",
    "    output_file = FACT_DIR / \"factAidTransactions.csv\"\n",
    "    fact_table.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n Created fact table DataFrame with {len(fact_table):,} rows\")\n",
    "    return fact_table\n",
    "\n",
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DIMENSION TABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dimensions = {}\n",
    "\n",
    "dimensions['time'] = create_dim_time(all_cleaned_data)\n",
    "dimensions['aid_type'] = create_dim_aid_type(all_cleaned_data)\n",
    "dimensions['finance_type'] = create_dim_finance_type(all_cleaned_data)\n",
    "dimensions['flow_type'] = create_dim_flow_type(all_cleaned_data)\n",
    "dimensions['task'] = create_dim_task(all_cleaned_data)\n",
    "dimensions['provider_org'] = create_dim_provider_org(all_cleaned_data)\n",
    "dimensions['reporting_org'] = create_dim_reporting_org(all_cleaned_data)\n",
    "dimensions['country'] = create_dim_recipient_country(all_cleaned_data)\n",
    "dimensions['recipient_org'] = create_dim_recipient_org(all_cleaned_data, dimensions['country'])\n",
    "dimensions['sector'] = create_dim_sector(all_cleaned_data)\n",
    "dimensions['sub_sector'] = create_dim_sub_sector(all_cleaned_data, dimensions['sector'])\n",
    "\n",
    "print(f\"\\n Created {len(dimensions)} dimension tables\")\n",
    "\n",
    "combined_data = pd.concat(all_cleaned_data, ignore_index=True)\n",
    "\n",
    "# Create fact table\n",
    "fact_table = create_fact_aid_transactions(all_cleaned_data, dimensions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n All tables created in:\")\n",
    "print(f\"  Dimensions: {DIMENSION_DIR}\")\n",
    "print(f\"  Facts: {FACT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.0.46)\n",
      "Requirement already satisfied: pyodbc in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sqlalchemy) (4.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWarning:\u001b[0m unixodbc 2.3.14 is already installed and up-to-date.\n",
      "To reinstall 2.3.14, run:\n",
      "  brew reinstall unixodbc\n",
      "\u001b[33mWarning:\u001b[0m No available formula with the name \"msodbcsql18\".\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSearching for similarly named formulae and casks...\u001b[0m\n",
      "\u001b[31mError:\u001b[0m No formulae or casks found for msodbcsql18.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy pyodbc\n",
    "!brew install unixodbc\n",
    "!brew install msodbcsql18\n",
    "\n",
    "#import the necessary library\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "All tables exported successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "username = 'sa'\n",
    "password = 'MyStr0ngPwd!2026'\n",
    "host = 'localhost'\n",
    "port = 1433\n",
    "database = 'financial_aid_db'\n",
    "\n",
    "connection_url = f\"mssql+pymssql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connection successful!\")\n",
    "\n",
    "        dimensions['time'].to_sql('dim_time', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['aid_type'].to_sql('dim_aid_type', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['finance_type'].to_sql('dim_finance_type', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['flow_type'].to_sql('dim_flow_type', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['task'].to_sql('dim_task', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['provider_org'].to_sql('dim_provider_org', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['reporting_org'].to_sql('dim_reporting_org', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['country'].to_sql('dim_recipient_country', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['recipient_org'].to_sql('dim_recipient_org', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['sector'].to_sql('dim_sector', con=conn, if_exists='replace', index=False)\n",
    "        dimensions['sub_sector'].to_sql('dim_sub_sector', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "        fact_table.to_sql('fact_aid_transactions', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "        print(\"All tables exported successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227873"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fact_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
