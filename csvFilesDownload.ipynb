{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "countries = pd.read_csv(\"Countries1.csv\")\n",
    "country_codes = countries[\"Alpha2\"].tolist() # Get country code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Failed codes: ['JG', nan]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "base_url = \"https://data.countrydata.iatistandard.org/output/web/xlsx/en\"\n",
    "out_dir = Path(\"data/country_xlsx\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "failed = []\n",
    "\n",
    "for code in country_codes:\n",
    "    url = f\"{base_url}/{code}.xlsx\"\n",
    "    out_path = out_dir / f\"{code}.xlsx\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    except requests.exceptions.HTTPError:\n",
    "        failed.append(code)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Failed codes:\", failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IATI DATA CLEANING AND TRANSFORMATION\n",
      "============================================================\n",
      "Input directory: data/country_xlsx\n",
      "Output directory: data/cleaned_country_xlsx\n",
      "\n",
      "Processing: AF.xlsx\n",
      "  Initial rows: 133277\n",
      "  Removed columns: Humanitarian, Multi Country, URL, Value (EUR), Value (Local currrency)\n",
      "  WARNING: Found 6999 negative values in Value (USD)\n",
      "  Removed 9345 duplicate rows\n",
      "  Final rows: 123932\n",
      "  Saved to: AF_cleaned.csv\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = Path(\"data/country_xlsx\")\n",
    "OUTPUT_DIR = Path(\"data/cleaned_country_xlsx\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns to remove\n",
    "COLUMNS_TO_REMOVE = ['Humanitarian', 'Multi Country', 'URL', 'Value (EUR)', 'Value (Local currrency)']\n",
    "\n",
    "# Expected date format for database\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "\n",
    "def clean_text_field(text): # Standardize text fields: trim whitespace, normalize encoding\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    text = str(text).strip()\n",
    "\n",
    "    # Handle 'No data' or similar placeholders\n",
    "    if text.lower() in ['no data', 'n/a', 'na', 'null', '']:\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_country_name(country):\n",
    "    # Normalize country spellings and names\n",
    "    # Common corrections for misspellings\n",
    "\n",
    "    if pd.isna(country):\n",
    "        return country\n",
    "\n",
    "    country = str(country).strip()\n",
    "\n",
    "    # Dictionary of common misspellings/variations\n",
    "    corrections = {\n",
    "        'Turkiye': 'Turkey',\n",
    "        'TÃ¼rkiye': 'Turkey',\n",
    "        'Ivory Coast': \"CÃ´te d'Ivoire\",\n",
    "        'Congo DRC': 'Democratic Republic of the Congo',\n",
    "        'Congo Republic': 'Republic of the Congo',\n",
    "    }\n",
    "\n",
    "    for wrong, correct in corrections.items():\n",
    "        if wrong.lower() in country.lower():\n",
    "            return correct\n",
    "\n",
    "    return country\n",
    "\n",
    "\n",
    "def validate_numeric_values(df):\n",
    "    # Validate that numeric fields have acceptable values\n",
    "    # Remove/correct impossible values (e.g., negative values where positive expected)\n",
    "\n",
    "    # Value columns should not be negative (assuming all transactions are positive)\n",
    "    value_columns = ['Value (USD)']\n",
    "\n",
    "    for col in value_columns:\n",
    "        if col in df.columns:\n",
    "            # Flag negative values\n",
    "            negative_mask = df[col] < 0\n",
    "            if negative_mask.any():\n",
    "                print(f\"  WARNING: Found {negative_mask.sum()} negative values in {col}\")\n",
    "                # Option 1: Set to absolute value\n",
    "                # df.loc[negative_mask, col] = df.loc[negative_mask, col].abs()\n",
    "                # Option 2: Set to NaN\n",
    "                df.loc[negative_mask, col] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df): #Remove duplicate rows based on key columns\n",
    "\n",
    "    # Define key columns that should be unique\n",
    "    # Rows that are duplicated in all these columns will be considered duplicates\n",
    "    key_columns = [\n",
    "        'IATI Identifier',\n",
    "        'Transaction Type',\n",
    "        'Calendar Year',\n",
    "        'Calendar Quarter',\n",
    "        'Value (USD)'\n",
    "    ]\n",
    "\n",
    "    # Check columns exist\n",
    "    existing_keys = [col for col in key_columns if col in df.columns]\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=existing_keys, keep='first')\n",
    "    removed = initial_count - len(df)\n",
    "\n",
    "    if removed > 0:\n",
    "        print(f\"  Removed {removed} duplicate rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_transform_data(file_path):\n",
    "    # Main cleaning and transformation function for a single country file\n",
    "\n",
    "    country_code = file_path.stem  # e.g., 'AF' from 'AF.xlsx'\n",
    "    print(f\"\\nProcessing: {country_code}.xlsx\")\n",
    "\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"  Initial rows: {len(df)}\")\n",
    "\n",
    "        # 1. Remove specified columns\n",
    "        columns_to_drop = [col for col in COLUMNS_TO_REMOVE if col in df.columns]\n",
    "        if columns_to_drop:\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "            print(f\"  Removed columns: {', '.join(columns_to_drop)}\")\n",
    "\n",
    "        # 2. Clean text fields\n",
    "        text_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].apply(clean_text_field)\n",
    "\n",
    "        # 3. Normalize country names\n",
    "        if 'Recipient Country or Region' in df.columns:\n",
    "            df['Recipient Country or Region'] = df['Recipient Country or Region'].apply(\n",
    "                normalize_country_name\n",
    "            )\n",
    "\n",
    "        # 4. Validate numeric values\n",
    "        df = validate_numeric_values(df)\n",
    "\n",
    "        # 5. Remove duplicates\n",
    "        df = remove_duplicates(df)\n",
    "\n",
    "        # 6. Handle missing values in key fields\n",
    "        # Option: Drop rows with missing critical fields\n",
    "        critical_fields = ['IATI Identifier', 'Transaction Type', 'Calendar Year']\n",
    "        before_drop = len(df)\n",
    "        df = df.dropna(subset=critical_fields)\n",
    "        dropped = before_drop - len(df)\n",
    "        if dropped > 0:\n",
    "            print(f\"Dropped {dropped} rows with missing critical fields\")\n",
    "\n",
    "        # 7. Sort by date and identifier\n",
    "        sort_columns = ['Transaction Date', 'IATI Identifier']\n",
    "        existing_sort = [col for col in sort_columns if col in df.columns]\n",
    "        if existing_sort:\n",
    "            df = df.sort_values(by=existing_sort)\n",
    "\n",
    "        # 8. Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Save cleaned data\n",
    "        output_file = OUTPUT_DIR / f\"{country_code}_cleaned.csv\"\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"  Final rows: {len(df)}\")\n",
    "        print(f\"  Saved to: {output_file.name}\")\n",
    "\n",
    "        return {\n",
    "            'country': country_code,\n",
    "            'initial_rows': len(df),\n",
    "            'final_rows': len(df),\n",
    "            'success': True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")\n",
    "        return {\n",
    "            'country': country_code,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Main execution function\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"IATI DATA CLEANING AND TRANSFORMATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input directory: {INPUT_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Get all Excel files (country files)\n",
    "    excel_files = list(INPUT_DIR.glob(\"*.xlsx\"))\n",
    "    \n",
    "    file_path = INPUT_DIR / \"AF.xlsx\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"\\nERROR: File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    clean_and_transform_data(file_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values found: 0\n",
      "No negative values found ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to cleaned file\n",
    "clean_file = OUTPUT_DIR / \"AF_cleaned.csv\"   # adjust name if needed\n",
    "\n",
    "# Load cleaned data\n",
    "df_clean = pd.read_csv(clean_file)\n",
    "\n",
    "# Ensure value column is numeric\n",
    "df_clean['Value (USD)'] = pd.to_numeric(df_clean['Value (USD)'], errors='coerce')\n",
    "\n",
    "# Check for negative values\n",
    "negative_rows = df_clean[df_clean['Value (USD)'] < 0]\n",
    "\n",
    "print(f\"Negative values found: {len(negative_rows)}\")\n",
    "\n",
    "if not negative_rows.empty:\n",
    "    print(\"\\nSample negative-value rows:\")\n",
    "    print(\n",
    "        negative_rows[\n",
    "            ['IATI Identifier', 'Transaction Type', 'Transaction Date']\n",
    "        ].head()\n",
    "    )\n",
    "else:\n",
    "    print(\"No negative values found ðŸŽ‰\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
